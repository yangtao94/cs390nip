{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "# Setting random seeds to keep everything deterministic.\n",
    "random.seed(1618)\n",
    "np.random.seed(1618)\n",
    "tf.set_random_seed(1618)\n",
    "\n",
    "# Disable some troublesome logging.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Information on dataset.\n",
    "NUM_CLASSES = 10\n",
    "IMAGE_SIZE = 784\n",
    "\n",
    "# Use these to set the algorithm to use.\n",
    "#ALGORITHM = \"guesser\"\n",
    "ALGORITHM = \"custom_net\"\n",
    "#ALGORITHM = \"tf_net\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork_2Layer():\n",
    "    def __init__(self, inputSize, outputSize, neuronsPerLayer, learningRate = 0.01,addLayer = False,activation = \"sigmoid\"):\n",
    "        #initialize neural network with option to add in 3rd layer, and to change the activation function\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "        self.neuronsPerLayer = neuronsPerLayer\n",
    "        self.lr = learningRate\n",
    "        self.act = activation\n",
    "        self.addLayer = addLayer\n",
    "        if addLayer:\n",
    "            self.W1 = np.random.randn(self.inputSize,self.neuronsPerLayer)\n",
    "            self.W2 = np.random.randn(self.neuronsPerLayer,self.neuronsPerLayer)\n",
    "            self.W3 = np.random.randn(self.neuronsPerLayer,self.outputSize)\n",
    "        else:\n",
    "            self.W1 = np.random.randn(self.inputSize, self.neuronsPerLayer)\n",
    "            self.W2 = np.random.randn(self.neuronsPerLayer, self.outputSize)\n",
    "    \n",
    "    def activation_prime(self,x):\n",
    "        if self.act == \"sigmoid\":\n",
    "            return self.__sigmoidDerivative(x)\n",
    "        elif self.act == \"relu\":\n",
    "            return self.__reluDerivative(x)\n",
    "        else:\n",
    "            raise ValueError(\"Please select a valid activation function\")\n",
    "    def activation(self,x):\n",
    "        if self.act == \"sigmoid\":\n",
    "            return self.__sigmoid(x)\n",
    "        elif self.act == \"relu\":\n",
    "            return self.__relu(x)\n",
    "        else:\n",
    "            raise ValueError(\"Please select a valid activation function!\")\n",
    "    def __relu(self,x):\n",
    "        for i in x:\n",
    "            #print(i)\n",
    "            np.maximum(i,0,i)\n",
    "            #print(i)\n",
    "        return x\n",
    "    def __reluDerivative(self,x):\n",
    "        #print(\"before\",x[:1])\n",
    "        for i in x:\n",
    "            #print(i)\n",
    "            i[i<=0] = 0\n",
    "            i[i>0] = 1\n",
    "            #print(i)\n",
    "        #print(\"after\",x[:1])\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    # Activation function.\n",
    "    def __sigmoid(self, x):\n",
    "        return 1/(1+ np.exp(-x))\n",
    "\n",
    "    # Activation prime function.\n",
    "    def __sigmoidDerivative(self, x):\n",
    "        return x*(1-x)\n",
    "\n",
    "    # Batch generator for mini-batches. Not randomized.\n",
    "    def __batchGenerator(self, l, n):\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i : i + n]\n",
    "    \n",
    "    def backward(self, xVals, yVals,layers):\n",
    "        \n",
    "        d_o_errors = yVals - layers[-1]\n",
    "        d_o_delta = d_o_errors*(self.activation_prime(layers[-1]))\n",
    "        \n",
    "        if(len(layers) == 3):\n",
    "            d_w2_errors = d_o_delta.dot(self.W3.T)\n",
    "            d_w2_delta = d_w2_errors*(self.activation_prime(layers[-2]))\n",
    "        \n",
    "        \n",
    "            d_w1_errors = d_w2_delta.dot(self.W2.T)\n",
    "            d_w1_delta = d_w1_errors*(self.activation_prime(layers[0]))\n",
    "            self.W1 += self.lr * (xVals.T.dot(d_w1_delta))\n",
    "            self.W2 += self.lr * (layers[0].T.dot(d_w2_delta))\n",
    "            self.W3 += self.lr * (layers[1].T.dot(d_o_delta))\n",
    "        \n",
    "        #adjustment of the weights\n",
    "        else:\n",
    "            d_w2_errors = d_o_delta.dot(self.W2.T)\n",
    "            d_w2_delta = d_w2_errors*(self.__activation_prime(layers[-2]))\n",
    "            \n",
    "            \n",
    "            self.W1 += self.lr * (xVals.T.dot(d_w2_delta))\n",
    "            self.W2 += self.lr * (layers[0].T.dot(d_o_delta))\n",
    "        \n",
    "        \n",
    "    # Training with backpropagation.\n",
    "    def train(self, xVals, yVals, epochs = 10, minibatches = True, mbs = 100):\n",
    "        #print(\"Lakers\",xVals.shape[1])\n",
    "        y_length = yVals.shape[1]\n",
    "        #TODO: Implement backprop. allow minibatches. mbs should specify the size of each minibatch.\n",
    "        stuff = []\n",
    "        #forward pass\n",
    "        for i in range(epochs):\n",
    "            #mini-batches\n",
    "            print(\"iteration no: \",i)\n",
    "            l = np.concatenate((xVals,yVals),axis=1)\n",
    "            random.shuffle(l)\n",
    "           \n",
    "            #shuffle?\n",
    "            get_batches = self.__batchGenerator(l,mbs)\n",
    "            \n",
    "            for value in get_batches:\n",
    "                \n",
    "                \n",
    "                xVals_mini = value[:,:-y_length]\n",
    "                yVals_mini = value[:,-y_length:]\n",
    "                \n",
    "                stuff = self.__forward(xVals_mini)\n",
    "                #print(\"hihi\",len(stuff))\n",
    "                #backprop with minibatches\n",
    "                self.backward(xVals_mini,yVals_mini,stuff)\n",
    "        return stuff\n",
    "    \n",
    "     \n",
    "        \n",
    "        \n",
    "    # Forward pass.\n",
    "    def __forward(self, input):\n",
    "        #print(\"dimensions\",input.shape)\n",
    "        #print(\"dimensions\",self.W1.shape)\n",
    "        \n",
    "        net1 = np.dot(input,self.W1)\n",
    "        layer1 = self.activation(net1)\n",
    "        \n",
    "        if self.addLayer is True:\n",
    "            layer2 = self.activation(np.dot(layer1, self.W2))\n",
    "            layer3 = self.activation(np.dot(layer2,self.W3))\n",
    "            return layer1,layer2,layer3\n",
    "        layer2 = self.activation(np.dot(layer1, self.W2))\n",
    "        return layer1, layer2\n",
    "\n",
    "    # Predict.\n",
    "    def predict(self, xVals):\n",
    "        #return only the output layer\n",
    "        if self.addLayer is True:\n",
    "            _,_,layer3 = self.__forward(xVals)\n",
    "            return layer3\n",
    "        \n",
    "        _, layer2 = self.__forward(xVals)\n",
    "        return layer2\n",
    "\n",
    "    \n",
    "\n",
    "# Classifier that just guesses the class label.\n",
    "def guesserClassifier(xTest):\n",
    "    ans = []\n",
    "    for entry in xTest:\n",
    "        pred = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        pred[random.randint(0, 9)] = 1\n",
    "        ans.append(pred)\n",
    "    return np.array(ans)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#=========================<Pipeline Functions>==================================\n",
    "\n",
    "def getRawData():\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n",
    "    print(\"Shape of xTrain dataset: %s.\" % str(xTrain.shape))\n",
    "    print(\"Shape of yTrain dataset: %s.\" % str(yTrain.shape))\n",
    "    print(\"Shape of xTest dataset: %s.\" % str(xTest.shape))\n",
    "    print(\"Shape of yTest dataset: %s.\" % str(yTest.shape))\n",
    "    return ((xTrain, yTrain), (xTest, yTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessData(raw):\n",
    "    ((xTrain, yTrain), (xTest, yTest)) = raw            #TODO: Add range reduction here (0-255 ==> 0.0-1.0).\n",
    "    xTrain = xTrain.reshape(60000,IMAGE_SIZE)\n",
    "    xTest = xTest.reshape(10000,IMAGE_SIZE)\n",
    "    #range deduction\n",
    "    denom = 255\n",
    "    xTrain = xTrain / denom\n",
    "    xTest = xTest/denom\n",
    "    yTrainP = to_categorical(yTrain, NUM_CLASSES)\n",
    "    yTestP = to_categorical(yTest, NUM_CLASSES)\n",
    "    print(\"New shape of xTrain dataset: %s.\" % str(xTrain.shape))\n",
    "    print(\"New shape of xTest dataset: %s.\" % str(xTest.shape))\n",
    "    print(\"New shape of yTrain dataset: %s.\" % str(yTrainP.shape))\n",
    "    print(\"New shape of yTest dataset: %s.\" % str(yTestP.shape))\n",
    "    return ((xTrain, yTrainP), (xTest, yTestP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRawData_iris():\n",
    "    iris = datasets.load_iris()\n",
    "    attributes = iris.data\n",
    "    labels = iris.target\n",
    "    xTrain,xTest,yTrain,yTest = train_test_split(attributes,labels,test_size = 0.5)\n",
    "    return ((xTrain,yTrain),(xTest,yTest))\n",
    "def preprocess_iris(raw):\n",
    "    ((xTrain,yTrain),(xTest,yTest)) = raw\n",
    "    yTrainP = to_categorical(yTrain,3)\n",
    "    yTestP = to_categorical(yTest,3)\n",
    "    print(\"New shape of xTrain dataset: %s.\" % str(xTrain.shape))\n",
    "    print(\"New shape of xTest dataset: %s.\" % str(xTest.shape))\n",
    "    print(\"New shape of yTrain dataset: %s.\" % str(yTrainP.shape))\n",
    "    print(\"New shape of yTest dataset: %s.\" % str(yTestP.shape))\n",
    "    return ((xTrain,yTrainP),(xTest,yTestP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def trainModel(data):\n",
    "    xTrain, yTrain = data\n",
    "    if ALGORITHM == \"guesser\":\n",
    "        return None   # Guesser has no model, as it is just guessing.\n",
    "    elif ALGORITHM == \"custom_net\":\n",
    "        print(\"Building and training Custom_NN.\")\n",
    "        activation = [\"sigmoid\",\"relu\"]    \n",
    "        #TODO: Write code to build and train your custon neural net.\n",
    "        #initialize network\n",
    "        #set add layer to True\n",
    "        custom_net = NeuralNetwork_2Layer(xTrain.shape[1],yTrain.shape[1],30,0.1,True,activation[0])\n",
    "        \n",
    "        #Change batch size to 1 when testing the iris dataset\n",
    "        weights = custom_net.train(xTrain,yTrain,20,True,128)\n",
    "        print(\"trained!\")\n",
    "        return custom_net\n",
    "    elif ALGORITHM == \"tf_net\":\n",
    "        print(\"Building and training TF_NN.\")\n",
    "        #TODO: Write code to build and train your keras neural net.\n",
    "        model = tf.keras.Sequential()\n",
    "        lossType = tf.keras.losses.categorical_crossentropy\n",
    "        opt = tf.train.AdamOptimizer()\n",
    "        i_shape = (784,)\n",
    "        model.add(tf.keras.layers.Dense(512, input_shape = i_shape,activation = tf.nn.relu))\n",
    "        model.add(tf.keras.layers.Dropout(0.2))\n",
    "        model.add(tf.keras.layers.Dense(512,activation=tf.nn.relu))\n",
    "        model.add(tf.keras.layers.Dropout(0.2))\n",
    "        model.add(tf.keras.layers.Dense(10,activation = tf.nn.softmax))\n",
    "        model.compile(optimizer = opt, loss = lossType)\n",
    "        model.fit(xTrain,yTrain,batch_size = 128,epochs = 10)\n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError(\"Algorithm not recognized.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def runModel(data, model):\n",
    "    if ALGORITHM == \"guesser\":\n",
    "        return guesserClassifier(data)\n",
    "    elif ALGORITHM == \"custom_net\":\n",
    "        print(\"Testing Custom_NN.\")\n",
    "        #print(\"Not yet implemented.\")                   #TODO: Write code to run your custon neural net.\n",
    "        pred = model.predict(data)\n",
    "        \n",
    "        return pred\n",
    "    elif ALGORITHM == \"tf_net\":\n",
    "        print(\"Testing TF_NN.\")\n",
    "        #TODO: Write code to run your keras neural net.\n",
    "        preds = model.predict(data)\n",
    "        return preds\n",
    "    else:\n",
    "        raise ValueError(\"Algorithm not recognized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalResults(data, preds):   #TODO: Add F1 score confusion matrix here.\n",
    "    xTest, yTest = data\n",
    "    acc = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for i in range(preds.shape[0]):\n",
    "        y_pred.append(np.argmax(preds[i],0))\n",
    "        y_true.append(np.argmax(yTest[i],0))\n",
    "        if (np.argmax(preds[i],0) == np.argmax(yTest[i],0)):   acc = acc + 1\n",
    "    accuracy = acc / preds.shape[0]\n",
    "    #Print confusion matrix from sklearn\n",
    "    array = confusion_matrix(y_true,y_pred)\n",
    "    print(array)\n",
    "    print(\"Classifier algorithm: %s\" % ALGORITHM)\n",
    "    print(\"Classifier accuracy: %f%%\" % (accuracy * 100))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of xTrain dataset: (75, 4).\n",
      "New shape of xTest dataset: (75, 4).\n",
      "New shape of yTrain dataset: (75, 3).\n",
      "New shape of yTest dataset: (75, 3).\n",
      "Building and training Custom_NN.\n",
      "4 3\n",
      "iteration no:  0\n",
      "iteration no:  1\n",
      "iteration no:  2\n",
      "iteration no:  3\n",
      "iteration no:  4\n",
      "iteration no:  5\n",
      "iteration no:  6\n",
      "iteration no:  7\n",
      "iteration no:  8\n",
      "iteration no:  9\n",
      "iteration no:  10\n",
      "iteration no:  11\n",
      "iteration no:  12\n",
      "iteration no:  13\n",
      "iteration no:  14\n",
      "iteration no:  15\n",
      "iteration no:  16\n",
      "iteration no:  17\n",
      "iteration no:  18\n",
      "iteration no:  19\n",
      "trained!\n",
      "Testing Custom_NN.\n",
      "[[ 0  0 24]\n",
      " [ 0  0 27]\n",
      " [ 0  0 24]]\n",
      "Classifier algorithm: custom_net\n",
      "Classifier accuracy: 32.000000%\n",
      "\n",
      "Shape of xTrain dataset: (60000, 28, 28).\n",
      "Shape of yTrain dataset: (60000,).\n",
      "Shape of xTest dataset: (10000, 28, 28).\n",
      "Shape of yTest dataset: (10000,).\n",
      "New shape of xTrain dataset: (60000, 784).\n",
      "New shape of xTest dataset: (10000, 784).\n",
      "New shape of yTrain dataset: (60000, 10).\n",
      "New shape of yTest dataset: (10000, 10).\n",
      "Building and training Custom_NN.\n",
      "784 10\n",
      "iteration no:  0\n",
      "iteration no:  1\n",
      "iteration no:  2\n",
      "iteration no:  3\n",
      "iteration no:  4\n",
      "iteration no:  5\n",
      "iteration no:  6\n",
      "iteration no:  7\n",
      "iteration no:  8\n",
      "iteration no:  9\n",
      "iteration no:  10\n",
      "iteration no:  11\n",
      "iteration no:  12\n",
      "iteration no:  13\n",
      "iteration no:  14\n",
      "iteration no:  15\n",
      "iteration no:  16\n",
      "iteration no:  17\n",
      "iteration no:  18\n",
      "iteration no:  19\n",
      "trained!\n",
      "Testing Custom_NN.\n",
      "[[ 959    0    2    0    1    4    3    6    3    2]\n",
      " [   0 1116    4    3    0    1    1    2    7    1]\n",
      " [   7    2  968    9    7    8    6   12    8    5]\n",
      " [   2    1   24  949    2    8    0   10   12    2]\n",
      " [   2    1    2    0  938    1    7    6    6   19]\n",
      " [   7    5    3   30    4  816    9    3   10    5]\n",
      " [  10    3    5    0   12    5  913    1    8    1]\n",
      " [   2    9   18    2    7    1    2  963    4   20]\n",
      " [  11    7    5   17    5    5    6    7  900   11]\n",
      " [   7    4    2    7   32   11    1   12    9  924]]\n",
      "Classifier algorithm: custom_net\n",
      "Classifier accuracy: 94.460000%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#=========================<Main>================================================\n",
    "\n",
    "def main():\n",
    "    ##Uncomment when trying to test the iris_dataset\n",
    "    #change batch size to one when training the model\n",
    "#    if ALGORITHM != \"tf_net\":\n",
    "#        raw = getRawData_iris()\n",
    "#        data = preprocess_iris(raw)\n",
    "#        model = trainModel(data[0])\n",
    "#        preds = runModel(data[1][0],model)\n",
    "#        evalResults(data[1],preds)\n",
    "    \n",
    "    raw = getRawData()\n",
    "    data = preprocessData(raw)\n",
    "    model = trainModel(data[0])\n",
    "    preds = runModel(data[1][0], model)\n",
    "    evalResults(data[1], preds)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
